{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seinfeld TV Script Generator\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this project, we will generate our own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using recurrent neural networks (RNNs). We will be using part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from its 9 seasons, and the neural network we will build will generate a new,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "<img src='images/seinfeld_logo.png'>\n",
    "\n",
    "As a technical prerequisite, to perform that, first, we need to make some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed imports:\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seinfeld_unittests as tests\n",
    "import helper, os, time, torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.functional import softmax\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "\n",
    "The data is already provided in `data/seinfeld_scripts.txt`: As a first step, we will load in this data and look at some samples, afterward, we will define and train an RNN to generate a new script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data:\n",
    "data_dir = 'data/seinfeld_scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data\n",
    "\n",
    "Now, we are going to play around with `view_line_range` to view different parts of the data. This will give us a sense of the data we will be working with.\n",
    "\n",
    "As we are going to see, for example, the data is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dataset Stats:\n",
      "\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 6\n",
      "\n",
      "*** Dataset Sample:\n",
      "\n",
      "The lines 0 to 15:\n",
      "\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n",
      "jerry: oh, you dont recall? \n",
      "\n",
      "george: (on an imaginary microphone) uh, no, not at this time. \n",
      "\n",
      "jerry: well, senator, id just like to know, what you knew and when you knew it. \n"
     ]
    }
   ],
   "source": [
    "# Some stats on the dataset:\n",
    "\n",
    "print(\"*** Dataset Stats:\\n\")\n",
    "print(\"Roughly the number of unique words: {}\".format(len({word: None for word in text.split()})))\n",
    "lines = text.split('\\n')\n",
    "print(\"Number of lines: {}\".format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print(\"Average number of words in each line: {}\\n\".format(int(round(np.average(word_count_line)))))\n",
    "\n",
    "# Display a sample of the dataset:\n",
    "\n",
    "print(\"*** Dataset Sample:\\n\")\n",
    "view_line_range = (0, 15)\n",
    "print(\"The lines {} to {}:\\n\".format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "The first thing to do to any dataset is pre-processing: For this, here, we need to perform word embedding, and so, we are going to implement the following pre-processing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### *Lookup table*\n",
    "\n",
    "To create a word embedding, we first need to transform the words to ids, so, in this function, we will create two dictionaries:\n",
    "- Dictionary to go from a word to an id: we will call it `vocab_to_int`\n",
    "- Dictionary to go from an id to a word: we will call it `int_to_vocab`\n",
    "\n",
    "These dictionaries will be returned in the following tuple: `(vocab_to_int, int_to_vocab)`.\n",
    "\n",
    "*Nota Bene:* The integers are assigned in descending frequency order, so the most frequent word (probably \"the\") is given the integer 0 and the next most frequent is 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'create_lookup_tables' has been defined: Running some unit tests...\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Function to create lookup tables:\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    \"\"\"\n",
    "    # Create a Counter object from text:\n",
    "    word_counts = Counter(text)\n",
    "    # Sorting the words from most to least frequent in text occurrence:\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    # Create dictionaries:\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    # Return tuple:\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "# Perform some unit tests on the function:\n",
    "\n",
    "print(\"Function 'create_lookup_tables' has been defined: Running some unit tests...\")\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tokenize punctuation*\n",
    "\n",
    "We will be splitting the script into a word array using spaces as delimiters. However, punctuations like periods and exclamation marks can create multiple ids for the same word (e.g. \"bye\" and \"bye!\" would generate two different word ids).\n",
    "\n",
    "To tackle this issue, we are going to implement the function `token_lookup` to return a dictionary that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".\n",
    "\n",
    "First, we need to create a dictionary for the following symbols, where the symbol is the key and the value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parenthesis ( **(** )\n",
    "- Right Parenthesis ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it. This separates each symbol as its own word, making it easier for the neural network to predict the next word.\n",
    "\n",
    "*Nota Bene:* It is important to make sure that we don't use a value that could be confused as a word (e.g. instead of using the value \"dash\", it's better to use something like \"||DASH||\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'token_lookup' has been defined: Running some unit tests...\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Function to create punctuation dictionary:\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dictionary to turn punctuation into a token\n",
    "    \"\"\"\n",
    "    # Initialize dictionary:\n",
    "    token_dict = {}\n",
    "    # Fill dictionary:\n",
    "    token_dict['.'] = '||PERIOD||'\n",
    "    token_dict[','] = '||COMMA||'\n",
    "    token_dict['\"'] = '||QUOTATION_MARK||'\n",
    "    token_dict[';'] = '||SEMICOLON||'\n",
    "    token_dict['!'] = '||EXCLAMATION_MARK||'\n",
    "    token_dict['?'] = '||QUESTION_MARK||'\n",
    "    token_dict['('] = '||LEFT_PARENTHESIS||'\n",
    "    token_dict[')'] = '||RIGHT_PARENTHESIS||'\n",
    "    token_dict['-'] = '||DASH||'\n",
    "    token_dict['\\n'] = '||RETURN||'\n",
    "    # Return dictionary:\n",
    "    return token_dict\n",
    "\n",
    "# Perform some unit tests on the function:\n",
    "\n",
    "print(\"Function 'token_lookup' has been defined: Running some unit tests...\")\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save the pre-processed Data*\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it into the file `data/preprocessed_data.pickle` (for this, we use the `preprocess_and_save_data` function in the `helpers.py` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the data:\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Preprocessed data \n",
    "\n",
    "This is our first checkpoint: If we ever decide to come back to this notebook, or have to restart it, we can start from here, the preprocessed data have been saved to the disk.\n",
    "\n",
    "Running the code cell below will load the preprocessed data from the file `data/preprocessed_data.pickle` (for this, we use the `load_preprocess` function in the `helpers.py` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data:\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an RNN\n",
    "\n",
    "In this section, we will build an RNN by implementing the RNN module, and the forward and backpropagation functions.\n",
    "\n",
    "### *Check access to GPU*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found!\n"
     ]
    }
   ],
   "source": [
    "# Check for a GPU:\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print(\"GPU found!\")\n",
    "else:\n",
    "    print(\"No GPU found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Input*\n",
    "\n",
    "Let's start with the preprocessed input data: we will use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "We can create data with TensorDataset by passing in feature and target tensors, and then, by creating a DataLoader as usual.\n",
    "```\n",
    "data_set = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "```\n",
    "\n",
    "### *Batching*\n",
    "\n",
    "Now, we are going to implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    "*Nota Bene:* We can batch words using the DataLoader, but it will be up to us to create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`.\n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "Our first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "And the corresponding `target_tensor` should just be the next \"word/tokenized word\" value:\n",
    "```\n",
    "5\n",
    "```\n",
    "\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader from preprocessed data:\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    Parameters\n",
    "     words - the word ids of the TV scripts\n",
    "     sequence_length - the sequence length of each batch\n",
    "     batch_size - the size of each batch, the number of sequences in a batch\n",
    "    Returns\n",
    "     data_loader - DataLoader with batched data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the length of each batch:\n",
    "    batch_length = batch_size*sequence_length\n",
    "    \n",
    "    # Determine the number of batches:\n",
    "    n_batches = len(words)//batch_length\n",
    "    \n",
    "    # Restrict to only full batches:\n",
    "    restrict_length = n_batches*batch_length\n",
    "    words = words[:restrict_length]\n",
    "    \n",
    "    # Initialize features and targets:\n",
    "    feature_list = []\n",
    "    target_list = []\n",
    "    \n",
    "    # Build features and targets:\n",
    "    for idx in range(restrict_length - sequence_length):\n",
    "        feature_list.append(words[idx:idx+sequence_length])\n",
    "        target_list.append(words[idx+sequence_length])\n",
    "        \n",
    "    # Convert features and targets to tensors:\n",
    "    feature_tensors = torch.from_numpy(np.asarray(feature_list))\n",
    "    target_tensors = torch.from_numpy(np.asarray(target_list))\n",
    "    \n",
    "    # Build dataloader:\n",
    "    data_set = TensorDataset(feature_tensors, target_tensors)\n",
    "    data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Return dataloader:\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Test our dataloader* \n",
    "\n",
    "Below, we are going to generate some test text data and define a dataloader using the function we defined above.\n",
    "\n",
    "Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "Our code should return something like the following (likely in a different order: We shuffled our data):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### *Sizes*\n",
    "\n",
    "Our `sample_x` should be of size `(batch_size, sequence_length)`, or (10, 5) in this case, and `sample_y` should just have one dimension: batch_size (10). \n",
    "\n",
    "### *Values*\n",
    "\n",
    "We should also notice that the targets, `sample_y`, are the ***next*** value in the ordered test text data. So, e.g., for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results:\n",
      "\n",
      "*** Feature tensors\n",
      "torch.Size([10, 5])\n",
      "tensor([[  8,   9,  10,  11,  12],\n",
      "        [ 26,  27,  28,  29,  30],\n",
      "        [ 43,  44,  45,  46,  47],\n",
      "        [ 13,  14,  15,  16,  17],\n",
      "        [ 15,  16,  17,  18,  19],\n",
      "        [ 30,  31,  32,  33,  34],\n",
      "        [ 40,  41,  42,  43,  44],\n",
      "        [ 24,  25,  26,  27,  28],\n",
      "        [  9,  10,  11,  12,  13],\n",
      "        [ 38,  39,  40,  41,  42]])\n",
      "\n",
      "*** Target tensors\n",
      "torch.Size([10])\n",
      "tensor([ 13,  31,  48,  18,  20,  35,  45,  29,  14,  43])\n"
     ]
    }
   ],
   "source": [
    "# Test our dataloader:\n",
    "\n",
    "# Define and test a text:\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "# Print results:\n",
    "print(\"Test results:\")\n",
    "print(\"\\n*** Feature tensors\")\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print(\"\\n*** Target tensors\")\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results are OK!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define our RNN*\n",
    "\n",
    "Now, we are going to implement an RNN using PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). We have the possibility to choose to use a GRU or an LSTM: Here, we will choose an LSTM. Furthermore, to complete the RNN, we will have to implement the following functions for the class:\n",
    "- `__init__`: The initialize function.\n",
    "- `forward`: The forward propagation function.\n",
    "- `init_hidden`: The initialization function for an LSTM hidden state.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class, and the forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "*Nota Bene:* The output of this model should be the ***last*** batch of word scores after the processing of a complete sequence. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n",
    "*Nota Bene:* Here are some points of attention we will focus on:\n",
    "- Make sure to stack the outputs of the LSTM to pass to our fully-connected layer, we can do this with:\n",
    "```\n",
    "lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "```\n",
    "- Make sure to get the last batch of word scores by shaping the output of the final, fully-connected layer like so:\n",
    "```\n",
    "# Reshape into (batch_size, seq_length, output_size):\n",
    "out = out.view(batch_size, -1, self.output_size)\n",
    "# Get last batch:\n",
    "last_out = out[:, -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'RNN' has been defined: Running some unit tests...\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Class RNN which is going to be used:\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        Parameters\n",
    "         vocab_size - the number of input dimensions of the neural network (the size of the vocabulary)\n",
    "         output_size - the number of output dimensions of the neural network\n",
    "         embedding_dim - the size of embeddings, should you choose to use them        \n",
    "         hidden_dim - the size of the hidden layer outputs\n",
    "         dropout - the dropout to add in between LSTM layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Set class variables:\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding and LSTM layers:\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Linear layer:\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        Parameters\n",
    "         nn_input - the input to the neural network\n",
    "         hidden - the hidden state \n",
    "        Returns\n",
    "         last_out, hidden - the output of the neural network, the last hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        # Embedding and LSTM Output:\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Fully-connected layer:\n",
    "        out = self.fc(lstm_out)\n",
    "        \n",
    "        # Reshape into (batch_size, seq_length, output_size):\n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        \n",
    "        # Get last batch:\n",
    "        last_out = out[:, -1]\n",
    "\n",
    "        # Return one batch of output word scores and the hidden state:\n",
    "        return last_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state of an LSTM\n",
    "        Parameters\n",
    "         batch_size - the batch size of the hidden state\n",
    "        Returns\n",
    "         hidden - hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize hidden state with zero weights, and move to GPU if available:\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "# Perform some unit tests on the class:\n",
    "\n",
    "print(\"Class 'RNN' has been defined: Running some unit tests...\")\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Define forward and backpropagation*\n",
    "\n",
    "Now, we are going to use the RNN class we have just implemented to apply forward and backpropagation.\n",
    "\n",
    "This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)` (we can get this loss by computing it, as usual, and calling `loss.item()`).\n",
    "\n",
    "*Nota Bene:* If a GPU is available, it is a good idea to move our data to that GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'forward_back_prop' has been defined: Running some unit tests...\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Function for forward and backpropagation:\n",
    "\n",
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    Parameters\n",
    "     rnn - the PyTorch module that holds the neural network\n",
    "     optimizer - the PyTorch optimizer for the neural network\n",
    "     criterion - the PyTorch loss function\n",
    "     inp - a batch of input to the neural network\n",
    "     target - the target output for the batch of input\n",
    "     hidden - the hidden state of the neural network\n",
    "    Returns\n",
    "     batch_loss, hidden_state - the loss over a batch, the last hidden state tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move data to GPU (if available):\n",
    "    if train_on_gpu:\n",
    "        rnn.cuda()\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # Perform backpropagation and optimization:\n",
    "    \n",
    "    # Backpropagation through the entire training history:\n",
    "    hidden_state = tuple([each.data for each in hidden])\n",
    "    \n",
    "    # Zero accumulated gradients:\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    # Get the output from the model:\n",
    "    output, hidden_state = rnn(inp, hidden_state)\n",
    "    \n",
    "    # Calculate the loss:\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Perform backpropagation:\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip_grad_norm_ helps prevent the exploding gradient problem in RNNs/LSTMs:\n",
    "    clip_grad_norm_(rnn.parameters(), max_norm=5)\n",
    "    \n",
    "    # Perform optimization:\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate loss over the batch:\n",
    "    batch_loss = loss.item()\n",
    "\n",
    "    # Return average loss over a batch and hidden state produced by the model:\n",
    "    return batch_loss, hidden_state\n",
    "\n",
    "# Perform some unit tests on the function:\n",
    "\n",
    "print(\"Function 'forward_back_prop' has been defined: Running some unit tests...\")\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the neural network complete, and data ready to be fed in the neural network, it's time to train it!\n",
    "\n",
    "### *Train Loop*\n",
    "\n",
    "The training loop is implemented below in the `train_rnn` function: This function will train the neural network over all the batches for the number of epochs given. The model progress will be shown every number of batches, and this number is set with the `show_every_n_batches` parameter (we will set this parameter along with other parameters in the next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to train our RNN:\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    \n",
    "    # Initialize losses:\n",
    "    batch_losses = []\n",
    "    \n",
    "    # Put model in training mode:\n",
    "    rnn.train()\n",
    "\n",
    "    # Track the training session:\n",
    "    print(\"Training of the neural network (for {} epoch(s)):\\n*****\".format(n_epochs))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a performance tracking file for the training session:\n",
    "    perf_file = open('training/training_perf.txt', 'w')\n",
    "    perf_file.write(\"# Epoch, Loss\\n\")\n",
    "    perf_file.close()\n",
    "    \n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # Initialize hidden state:\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # Make sure we iterate over completely full batches only:\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # Forward and backpropagation:\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
    "            \n",
    "            # Record loss:\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # Print and register loss stats:\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                average_loss = np.average(batch_losses)\n",
    "                if batch_i == show_every_n_batches:\n",
    "                    perf_file = open('training/training_perf.txt', 'a')\n",
    "                    perf_file.write(\"{} {:.2f}\\n\".format(epoch_i, average_loss))\n",
    "                    perf_file.close()\n",
    "                print(\"Epoch {}/{}, Step {}, Loss {:.2f}\".format(epoch_i,\n",
    "                                                                 n_epochs,\n",
    "                                                                 batch_i,\n",
    "                                                                 average_loss))\n",
    "                batch_losses = []\n",
    "                \n",
    "    # Time performance:\n",
    "    end_time = time.time()\n",
    "    total_time = int(end_time - start_time)\n",
    "    hours = total_time//3600\n",
    "    minutes = (total_time%3600)//60\n",
    "    seconds = (total_time%3600)%60\n",
    "    print(\"*****\\nEnd of the training: {:02d}h {:02d}m {:02d}s\".format(hours,\n",
    "                                                                       minutes,\n",
    "                                                                       seconds))\n",
    "\n",
    "    # Returns the trained RNN model:\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Hyperparameters*\n",
    "\n",
    "To train our neural network, we need to set the following hyperparameters:\n",
    "- `sequence_length`: the length of a sequence.\n",
    "- `batch_size`: the batch size.\n",
    "- `num_epochs`: the number of epochs to train for.\n",
    "- `learning_rate` the learning rate for an Adam optimizer.\n",
    "- `vocab_size`: the number of uniqe tokens in our vocabulary.\n",
    "- `output_size`: the desired size of the output.\n",
    "- `embedding_dim`: the embedding dimension (n.b. smaller than the vocab_size).\n",
    "- `hidden_dim`: the hidden dimension of our RNN.\n",
    "- `n_layers`: the number of layers/cells in our RNN.\n",
    "- `show_every_n_batches`: the number of batches at which the neural network should print progress.\n",
    "\n",
    "As can be seen in the next two cells, these hyperparameters can be divided into two categories:\n",
    "- Data hyperparameters.\n",
    "- Training hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data hyperparameters:\n",
    "\n",
    "# Sequence length:\n",
    "sequence_length = 50\n",
    "# Batch size:\n",
    "batch_size = 128\n",
    "\n",
    "# Data loader:\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters:\n",
    "\n",
    "# Number of epochs:\n",
    "num_epochs = 20\n",
    "# Learning rate:\n",
    "learning_rate = 0.001\n",
    "# Vocab size:\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size:\n",
    "output_size = vocab_size\n",
    "# Embedding dimension:\n",
    "embedding_dim = 300\n",
    "# Hidden dimension:\n",
    "hidden_dim = 256\n",
    "# Number of RNN layers:\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches:\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Training session*\n",
    "\n",
    "In the next cell, we will train the neural network on the pre-processed data: Here, our goal is to obtain a loss less than **3.5** (an arbitrary value).\n",
    "\n",
    "*Nota Bene:* Having a hard time getting a good loss is a *normal* thing. In such a situation, the solution consists of changing the hyperparameters. In general, better results can be obtained with larger hidden cells and layers dimensions, but these larger models take a longer time to train. It is equally a good idea to experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder 'training' to store training elements:\n",
    "os.mkdir('training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of the neural network (for 20 epoch(s)):\n",
      "*****\n",
      "Epoch 1/20, Step 500, Loss 5.52\n",
      "Epoch 1/20, Step 1000, Loss 4.85\n",
      "Epoch 1/20, Step 1500, Loss 4.63\n",
      "Epoch 1/20, Step 2000, Loss 4.52\n",
      "Epoch 1/20, Step 2500, Loss 4.44\n",
      "Epoch 1/20, Step 3000, Loss 4.35\n",
      "Epoch 1/20, Step 3500, Loss 4.32\n",
      "Epoch 1/20, Step 4000, Loss 4.26\n",
      "Epoch 1/20, Step 4500, Loss 4.26\n",
      "Epoch 1/20, Step 5000, Loss 4.20\n",
      "Epoch 1/20, Step 5500, Loss 4.18\n",
      "Epoch 1/20, Step 6000, Loss 4.14\n",
      "Epoch 1/20, Step 6500, Loss 4.14\n",
      "Epoch 2/20, Step 500, Loss 4.02\n",
      "Epoch 2/20, Step 1000, Loss 3.94\n",
      "Epoch 2/20, Step 1500, Loss 3.93\n",
      "Epoch 2/20, Step 2000, Loss 3.95\n",
      "Epoch 2/20, Step 2500, Loss 3.92\n",
      "Epoch 2/20, Step 3000, Loss 3.93\n",
      "Epoch 2/20, Step 3500, Loss 3.89\n",
      "Epoch 2/20, Step 4000, Loss 3.89\n",
      "Epoch 2/20, Step 4500, Loss 3.91\n",
      "Epoch 2/20, Step 5000, Loss 3.88\n",
      "Epoch 2/20, Step 5500, Loss 3.91\n",
      "Epoch 2/20, Step 6000, Loss 3.88\n",
      "Epoch 2/20, Step 6500, Loss 3.87\n",
      "Epoch 3/20, Step 500, Loss 3.79\n",
      "Epoch 3/20, Step 1000, Loss 3.71\n",
      "Epoch 3/20, Step 1500, Loss 3.72\n",
      "Epoch 3/20, Step 2000, Loss 3.72\n",
      "Epoch 3/20, Step 2500, Loss 3.72\n",
      "Epoch 3/20, Step 3000, Loss 3.72\n",
      "Epoch 3/20, Step 3500, Loss 3.72\n",
      "Epoch 3/20, Step 4000, Loss 3.71\n",
      "Epoch 3/20, Step 4500, Loss 3.73\n",
      "Epoch 3/20, Step 5000, Loss 3.73\n",
      "Epoch 3/20, Step 5500, Loss 3.74\n",
      "Epoch 3/20, Step 6000, Loss 3.72\n",
      "Epoch 3/20, Step 6500, Loss 3.74\n",
      "Epoch 4/20, Step 500, Loss 3.62\n",
      "Epoch 4/20, Step 1000, Loss 3.57\n",
      "Epoch 4/20, Step 1500, Loss 3.57\n",
      "Epoch 4/20, Step 2000, Loss 3.59\n",
      "Epoch 4/20, Step 2500, Loss 3.60\n",
      "Epoch 4/20, Step 3000, Loss 3.59\n",
      "Epoch 4/20, Step 3500, Loss 3.60\n",
      "Epoch 4/20, Step 4000, Loss 3.61\n",
      "Epoch 4/20, Step 4500, Loss 3.62\n",
      "Epoch 4/20, Step 5000, Loss 3.61\n",
      "Epoch 4/20, Step 5500, Loss 3.64\n",
      "Epoch 4/20, Step 6000, Loss 3.60\n",
      "Epoch 4/20, Step 6500, Loss 3.63\n",
      "Epoch 5/20, Step 500, Loss 3.54\n",
      "Epoch 5/20, Step 1000, Loss 3.46\n",
      "Epoch 5/20, Step 1500, Loss 3.48\n",
      "Epoch 5/20, Step 2000, Loss 3.47\n",
      "Epoch 5/20, Step 2500, Loss 3.51\n",
      "Epoch 5/20, Step 3000, Loss 3.50\n",
      "Epoch 5/20, Step 3500, Loss 3.52\n",
      "Epoch 5/20, Step 4000, Loss 3.51\n",
      "Epoch 5/20, Step 4500, Loss 3.51\n",
      "Epoch 5/20, Step 5000, Loss 3.54\n",
      "Epoch 5/20, Step 5500, Loss 3.54\n",
      "Epoch 5/20, Step 6000, Loss 3.52\n",
      "Epoch 5/20, Step 6500, Loss 3.56\n",
      "Epoch 6/20, Step 500, Loss 3.47\n",
      "Epoch 6/20, Step 1000, Loss 3.39\n",
      "Epoch 6/20, Step 1500, Loss 3.40\n",
      "Epoch 6/20, Step 2000, Loss 3.42\n",
      "Epoch 6/20, Step 2500, Loss 3.42\n",
      "Epoch 6/20, Step 3000, Loss 3.45\n",
      "Epoch 6/20, Step 3500, Loss 3.44\n",
      "Epoch 6/20, Step 4000, Loss 3.45\n",
      "Epoch 6/20, Step 4500, Loss 3.45\n",
      "Epoch 6/20, Step 5000, Loss 3.45\n",
      "Epoch 6/20, Step 5500, Loss 3.46\n",
      "Epoch 6/20, Step 6000, Loss 3.46\n",
      "Epoch 6/20, Step 6500, Loss 3.47\n",
      "Epoch 7/20, Step 500, Loss 3.40\n",
      "Epoch 7/20, Step 1000, Loss 3.32\n",
      "Epoch 7/20, Step 1500, Loss 3.35\n",
      "Epoch 7/20, Step 2000, Loss 3.34\n",
      "Epoch 7/20, Step 2500, Loss 3.36\n",
      "Epoch 7/20, Step 3000, Loss 3.36\n",
      "Epoch 7/20, Step 3500, Loss 3.37\n",
      "Epoch 7/20, Step 4000, Loss 3.38\n",
      "Epoch 7/20, Step 4500, Loss 3.38\n",
      "Epoch 7/20, Step 5000, Loss 3.41\n",
      "Epoch 7/20, Step 5500, Loss 3.42\n",
      "Epoch 7/20, Step 6000, Loss 3.42\n",
      "Epoch 7/20, Step 6500, Loss 3.45\n",
      "Epoch 8/20, Step 500, Loss 3.33\n",
      "Epoch 8/20, Step 1000, Loss 3.27\n",
      "Epoch 8/20, Step 1500, Loss 3.28\n",
      "Epoch 8/20, Step 2000, Loss 3.30\n",
      "Epoch 8/20, Step 2500, Loss 3.31\n",
      "Epoch 8/20, Step 3000, Loss 3.31\n",
      "Epoch 8/20, Step 3500, Loss 3.33\n",
      "Epoch 8/20, Step 4000, Loss 3.32\n",
      "Epoch 8/20, Step 4500, Loss 3.34\n",
      "Epoch 8/20, Step 5000, Loss 3.36\n",
      "Epoch 8/20, Step 5500, Loss 3.35\n",
      "Epoch 8/20, Step 6000, Loss 3.36\n",
      "Epoch 8/20, Step 6500, Loss 3.37\n",
      "Epoch 9/20, Step 500, Loss 3.30\n",
      "Epoch 9/20, Step 1000, Loss 3.23\n",
      "Epoch 9/20, Step 1500, Loss 3.23\n",
      "Epoch 9/20, Step 2000, Loss 3.24\n",
      "Epoch 9/20, Step 2500, Loss 3.26\n",
      "Epoch 9/20, Step 3000, Loss 3.29\n",
      "Epoch 9/20, Step 3500, Loss 3.27\n",
      "Epoch 9/20, Step 4000, Loss 3.28\n",
      "Epoch 9/20, Step 4500, Loss 3.30\n",
      "Epoch 9/20, Step 5000, Loss 3.29\n",
      "Epoch 9/20, Step 5500, Loss 3.33\n",
      "Epoch 9/20, Step 6000, Loss 3.32\n",
      "Epoch 9/20, Step 6500, Loss 3.31\n",
      "Epoch 10/20, Step 500, Loss 3.25\n",
      "Epoch 10/20, Step 1000, Loss 3.17\n",
      "Epoch 10/20, Step 1500, Loss 3.20\n",
      "Epoch 10/20, Step 2000, Loss 3.21\n",
      "Epoch 10/20, Step 2500, Loss 3.23\n",
      "Epoch 10/20, Step 3000, Loss 3.24\n",
      "Epoch 10/20, Step 3500, Loss 3.24\n",
      "Epoch 10/20, Step 4000, Loss 3.25\n",
      "Epoch 10/20, Step 4500, Loss 3.25\n",
      "Epoch 10/20, Step 5000, Loss 3.28\n",
      "Epoch 10/20, Step 5500, Loss 3.27\n",
      "Epoch 10/20, Step 6000, Loss 3.30\n",
      "Epoch 10/20, Step 6500, Loss 3.29\n",
      "Epoch 11/20, Step 500, Loss 3.22\n",
      "Epoch 11/20, Step 1000, Loss 3.15\n",
      "Epoch 11/20, Step 1500, Loss 3.15\n",
      "Epoch 11/20, Step 2000, Loss 3.18\n",
      "Epoch 11/20, Step 2500, Loss 3.21\n",
      "Epoch 11/20, Step 3000, Loss 3.20\n",
      "Epoch 11/20, Step 3500, Loss 3.22\n",
      "Epoch 11/20, Step 4000, Loss 3.20\n",
      "Epoch 11/20, Step 4500, Loss 3.23\n",
      "Epoch 11/20, Step 5000, Loss 3.25\n",
      "Epoch 11/20, Step 5500, Loss 3.23\n",
      "Epoch 11/20, Step 6000, Loss 3.25\n",
      "Epoch 11/20, Step 6500, Loss 3.26\n",
      "Epoch 12/20, Step 500, Loss 3.20\n",
      "Epoch 12/20, Step 1000, Loss 3.12\n",
      "Epoch 12/20, Step 1500, Loss 3.14\n",
      "Epoch 12/20, Step 2000, Loss 3.16\n",
      "Epoch 12/20, Step 2500, Loss 3.15\n",
      "Epoch 12/20, Step 3000, Loss 3.16\n",
      "Epoch 12/20, Step 3500, Loss 3.19\n",
      "Epoch 12/20, Step 4000, Loss 3.19\n",
      "Epoch 12/20, Step 4500, Loss 3.21\n",
      "Epoch 12/20, Step 5000, Loss 3.21\n",
      "Epoch 12/20, Step 5500, Loss 3.23\n",
      "Epoch 12/20, Step 6000, Loss 3.23\n",
      "Epoch 12/20, Step 6500, Loss 3.24\n",
      "Epoch 13/20, Step 500, Loss 3.17\n",
      "Epoch 13/20, Step 1000, Loss 3.10\n",
      "Epoch 13/20, Step 1500, Loss 3.11\n",
      "Epoch 13/20, Step 2000, Loss 3.11\n",
      "Epoch 13/20, Step 2500, Loss 3.13\n",
      "Epoch 13/20, Step 3000, Loss 3.15\n",
      "Epoch 13/20, Step 3500, Loss 3.16\n",
      "Epoch 13/20, Step 4000, Loss 3.17\n",
      "Epoch 13/20, Step 4500, Loss 3.17\n",
      "Epoch 13/20, Step 5000, Loss 3.18\n",
      "Epoch 13/20, Step 5500, Loss 3.21\n",
      "Epoch 13/20, Step 6000, Loss 3.21\n",
      "Epoch 13/20, Step 6500, Loss 3.24\n",
      "Epoch 14/20, Step 500, Loss 3.15\n",
      "Epoch 14/20, Step 1000, Loss 3.07\n",
      "Epoch 14/20, Step 1500, Loss 3.09\n",
      "Epoch 14/20, Step 2000, Loss 3.10\n",
      "Epoch 14/20, Step 2500, Loss 3.13\n",
      "Epoch 14/20, Step 3000, Loss 3.11\n",
      "Epoch 14/20, Step 3500, Loss 3.13\n",
      "Epoch 14/20, Step 4000, Loss 3.15\n",
      "Epoch 14/20, Step 4500, Loss 3.17\n",
      "Epoch 14/20, Step 5000, Loss 3.15\n",
      "Epoch 14/20, Step 5500, Loss 3.18\n",
      "Epoch 14/20, Step 6000, Loss 3.17\n",
      "Epoch 14/20, Step 6500, Loss 3.21\n",
      "Epoch 15/20, Step 500, Loss 3.12\n",
      "Epoch 15/20, Step 1000, Loss 3.06\n",
      "Epoch 15/20, Step 1500, Loss 3.07\n",
      "Epoch 15/20, Step 2000, Loss 3.07\n",
      "Epoch 15/20, Step 2500, Loss 3.10\n",
      "Epoch 15/20, Step 3000, Loss 3.10\n",
      "Epoch 15/20, Step 3500, Loss 3.10\n",
      "Epoch 15/20, Step 4000, Loss 3.12\n",
      "Epoch 15/20, Step 4500, Loss 3.14\n",
      "Epoch 15/20, Step 5000, Loss 3.15\n",
      "Epoch 15/20, Step 5500, Loss 3.16\n",
      "Epoch 15/20, Step 6000, Loss 3.17\n",
      "Epoch 15/20, Step 6500, Loss 3.18\n",
      "Epoch 16/20, Step 500, Loss 3.09\n",
      "Epoch 16/20, Step 1000, Loss 3.04\n",
      "Epoch 16/20, Step 1500, Loss 3.05\n",
      "Epoch 16/20, Step 2000, Loss 3.06\n",
      "Epoch 16/20, Step 2500, Loss 3.08\n",
      "Epoch 16/20, Step 3000, Loss 3.07\n",
      "Epoch 16/20, Step 3500, Loss 3.11\n",
      "Epoch 16/20, Step 4000, Loss 3.10\n",
      "Epoch 16/20, Step 4500, Loss 3.12\n",
      "Epoch 16/20, Step 5000, Loss 3.15\n",
      "Epoch 16/20, Step 5500, Loss 3.14\n",
      "Epoch 16/20, Step 6000, Loss 3.14\n",
      "Epoch 16/20, Step 6500, Loss 3.16\n",
      "Epoch 17/20, Step 500, Loss 3.08\n",
      "Epoch 17/20, Step 1000, Loss 3.03\n",
      "Epoch 17/20, Step 1500, Loss 3.04\n",
      "Epoch 17/20, Step 2000, Loss 3.05\n",
      "Epoch 17/20, Step 2500, Loss 3.04\n",
      "Epoch 17/20, Step 3000, Loss 3.09\n",
      "Epoch 17/20, Step 3500, Loss 3.07\n",
      "Epoch 17/20, Step 4000, Loss 3.09\n",
      "Epoch 17/20, Step 4500, Loss 3.08\n",
      "Epoch 17/20, Step 5000, Loss 3.11\n",
      "Epoch 17/20, Step 5500, Loss 3.11\n",
      "Epoch 17/20, Step 6000, Loss 3.14\n",
      "Epoch 17/20, Step 6500, Loss 3.15\n",
      "Epoch 18/20, Step 500, Loss 3.07\n",
      "Epoch 18/20, Step 1000, Loss 3.00\n",
      "Epoch 18/20, Step 1500, Loss 3.01\n",
      "Epoch 18/20, Step 2000, Loss 3.00\n",
      "Epoch 18/20, Step 2500, Loss 3.05\n",
      "Epoch 18/20, Step 3000, Loss 3.04\n",
      "Epoch 18/20, Step 3500, Loss 3.05\n",
      "Epoch 18/20, Step 4000, Loss 3.08\n",
      "Epoch 18/20, Step 4500, Loss 3.10\n",
      "Epoch 18/20, Step 5000, Loss 3.11\n",
      "Epoch 18/20, Step 5500, Loss 3.11\n",
      "Epoch 18/20, Step 6000, Loss 3.11\n",
      "Epoch 18/20, Step 6500, Loss 3.12\n",
      "Epoch 19/20, Step 500, Loss 3.04\n",
      "Epoch 19/20, Step 1000, Loss 2.98\n",
      "Epoch 19/20, Step 1500, Loss 3.01\n",
      "Epoch 19/20, Step 2000, Loss 3.01\n",
      "Epoch 19/20, Step 2500, Loss 3.03\n",
      "Epoch 19/20, Step 3000, Loss 3.03\n",
      "Epoch 19/20, Step 3500, Loss 3.04\n",
      "Epoch 19/20, Step 4000, Loss 3.07\n",
      "Epoch 19/20, Step 4500, Loss 3.07\n",
      "Epoch 19/20, Step 5000, Loss 3.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Step 5500, Loss 3.09\n",
      "Epoch 19/20, Step 6000, Loss 3.11\n",
      "Epoch 19/20, Step 6500, Loss 3.11\n",
      "Epoch 20/20, Step 500, Loss 3.04\n",
      "Epoch 20/20, Step 1000, Loss 2.98\n",
      "Epoch 20/20, Step 1500, Loss 2.98\n",
      "Epoch 20/20, Step 2000, Loss 3.02\n",
      "Epoch 20/20, Step 2500, Loss 3.00\n",
      "Epoch 20/20, Step 3000, Loss 3.00\n",
      "Epoch 20/20, Step 3500, Loss 3.03\n",
      "Epoch 20/20, Step 4000, Loss 3.03\n",
      "Epoch 20/20, Step 4500, Loss 3.05\n",
      "Epoch 20/20, Step 5000, Loss 3.07\n",
      "Epoch 20/20, Step 5500, Loss 3.08\n",
      "Epoch 20/20, Step 6000, Loss 3.09\n",
      "Epoch 20/20, Step 6500, Loss 3.09\n",
      "*****\n",
      "End of the training: 06h 32m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Model trained and saved!\n"
     ]
    }
   ],
   "source": [
    "# Training session:\n",
    "\n",
    "# Create model and move to gpu if available:\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# Define loss and optimization functions for training:\n",
    "optimizer = Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Train the model:\n",
    "with active_session():\n",
    "    trained_rnn = train_rnn(rnn,\n",
    "                            batch_size,\n",
    "                            optimizer,\n",
    "                            criterion,\n",
    "                            num_epochs,\n",
    "                            show_every_n_batches)\n",
    "\n",
    "# Save the trained model:\n",
    "helper.save_model('training/trained_rnn.pth', trained_rnn)\n",
    "print(\"=> Model trained and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2e1d17940>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 392
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Represent the training performances:\n",
    "\n",
    "# Read the data contained in the performance file:\n",
    "\n",
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "with open('training/training_perf.txt', 'r') as performance_file:\n",
    "    f_line = performance_file.readline()\n",
    "    while f_line != \"\":\n",
    "        if f_line[0] != \"#\":\n",
    "            f_line_l = f_line.strip().split()\n",
    "            epoch_list.append(int(f_line_l[0]))\n",
    "            loss_list.append(float(f_line_l[1]))\n",
    "        f_line = performance_file.readline()\n",
    "\n",
    "# Define the graph:\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Configuration of the graph:\n",
    "ax.set_title('Loss graph')\n",
    "ax.plot(epoch_list, loss_list, color='blue', linestyle='solid',\\\n",
    "        label='Loss Dataset')\n",
    "ax.plot([0, 20], [3.5, 3.5], color='black', linestyle='--',\\\n",
    "        linewidth=2, label='Loss Threshold')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 20)\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(21))\n",
    "ax.grid(color=\"grey\", which=\"major\", axis='x', linestyle='solid', linewidth=0.25)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(3., 6.)\n",
    "ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(7))\n",
    "ax.yaxis.set_minor_locator(mpl.ticker.MaxNLocator(13))\n",
    "ax.grid(color=\"grey\", which=\"major\", axis='y', linestyle='solid', linewidth=0.25)\n",
    "ax.grid(color=\"grey\", which=\"minor\", axis='y', linestyle='solid', linewidth=0.25)\n",
    "ax.set_ylabel('Loss')\n",
    "\n",
    "# Save the training graph:\n",
    "fig.savefig('training/training_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Setting of the hyperparameters*\n",
    "\n",
    "As it has been specified above, here, we have numerous hyperparameters we can tune to make better our model: In order to leave this notebook clear, I made the decision not to show the different tests that led me to make these choices, nevertheless, below, I will clarify some aspects of my thinking.\n",
    "\n",
    "As Yoshua Bengio (http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html) said, the learning rate is the single most important hyperparameter, and one should always make sure it has been tuned. A good starting point is 0.01, and other good options to consider are 0.1, 0.001, 0.0001 or 0.00001. The better results I have obtained have been for a learning rate set to 0.001.\n",
    "\n",
    "Respect to the sequence length hyperparameter, I have tried length from 10 to 100, and for the hidden dimension hyperparameter, I have tried the values 128, 256 and 512. The smaller these hyperparameters are, the lower the loss generally is, but the longer the training generally is too! To balance that situation, I decided to set the sequence length to 50 and the hidden dimension to 256.\n",
    "\n",
    "Finally, experience shows that for these types of architecture (RNNs), generally, 2 or 3 layers neural networks allow to achieve good performances, but going even deeper (4, 5, 6... layers) rarely helps much more (which is stark contrast to CNNs, where depth has been found to be an extremely important component for a good recognition system). So, to manage to have good performances and save training time, I have decided to use 2 layers (well, nevertheless, the training has lasted more than 6 hours on GPU mode!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint: Trained RNN model\n",
    "\n",
    "After running the above training cell, our model has been saved as `trained_rnn.pth` in the folder `training`.\n",
    "\n",
    "If we save our notebook progress, we can pause here and come back to this code at another time: Our progress can be resumed by running the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data and trained RNN model:\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('training/trained_rnn.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "\n",
    "With the neural network trained and saved, we are going to use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### *Generate Text*\n",
    "\n",
    "To generate the text, the neural network needs to start with a single word and repeat its predictions until it reaches a set length. We will be using the `generate` function to do this, it takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses top-k sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the text:\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    Parameters\n",
    "     rnn - the PyTorch module that holds the trained neural network\n",
    "     prime_id - the word id to start the first prediction\n",
    "     int_to_vocab - the dictionary of word id keys to word values\n",
    "     token_dict - the dictionary of puncuation tokens keys to puncuation values\n",
    "     pad_value - the value used to pad a sequence\n",
    "     predict_len - the length of text to generate\n",
    "    Returns\n",
    "     gen_sentences - the generated text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Put model in evaluation mode:\n",
    "    rnn.eval()\n",
    "    \n",
    "    # Create a sequence (batch_size=1) with the prime_id:\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # Initialize the hidden state:\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # Get the output of the RNN:\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # Get the next word probabilities:\n",
    "        proba = softmax(output, dim=1).data\n",
    "        if train_on_gpu:\n",
    "            proba = proba.cpu()\n",
    "         \n",
    "        # Use top_k sampling to get the index of the next word:\n",
    "        top_k = 5\n",
    "        proba, top_i = proba.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # Select the likely next word index with some element of randomness:\n",
    "        proba = proba.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=proba/proba.sum())\n",
    "        \n",
    "        # Retrieve that word from the dictionary:\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # The generated word becomes the next \"current sequence\" and the cycle can continue:\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens:\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # Return all the sentences:\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Generate a new script*\n",
    "\n",
    "Now, it's time to generate the text! For this, we have to set `gen_length` to the length of TV script we want to generate and set `prime_word` to one of the following to start the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "Jerry | Elaine\n",
    "- | -\n",
    "<img src=\"images/jerry.jpg\"> | <img src=\"images/elaine.jpg\">\n",
    "\n",
    "George | Kramer\n",
    "- | -\n",
    "<img src=\"images/george.jpg\"> | <img src=\"images/kramer.jpg\">\n",
    "\n",
    "*Note Bene:* We can set the prime word to *any word* in our dictionary, but it's best to start with a name for generating a TV script (we can also start with any other names we find in the original text file!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: i can go to the bathroom.\n",
      "\n",
      "jerry: you know, you have a good time.\n",
      "\n",
      "kramer: well, i'm gonna go to the bathroom.\n",
      "\n",
      "jerry: yeah.\n",
      "\n",
      "kramer: yeah, but it's a big deal.(sniffs)\n",
      "\n",
      "jerry: yeah.(to kramer) so i guess you're not getting married.\n",
      "\n",
      "kramer: well, that's the way i can do, you know, if you're not interested, and i don't know how to do that?\n",
      "\n",
      "jerry: i don't know what you're going to do.\n",
      "\n",
      "kramer: oh, no, no, no, no, no, no, no, no, i don't want to see it, you know, the usual. it's a very exciting place, no registration.\n",
      "\n",
      "george: i think it's sticking.\n",
      "\n",
      "george: well, i think i have no power.\n",
      "\n",
      "george: i know. you have to go. you know i don't know, but it's not really funny.(he exits)\n",
      "\n",
      "george: oh, yeah.\n",
      "\n",
      "susan: oh, hi.\n",
      "\n",
      "elaine:(looking at the closed door) what do you mean?\n",
      "\n",
      "elaine: i think i have a little problem.\n",
      "\n",
      "george:(looking at the wreck) you have no idea...\n",
      "\n",
      "george: i don't know if you could.(george nods)\n",
      "\n",
      "elaine:(looking at the door) oh yeah. yeah, that's right.(he opens the door)\n",
      "\n",
      "elaine:(quietly) oh... yeah?... yeah.\n",
      "\n",
      "jerry: hey, you guys haven't been in payroll.\n",
      "\n",
      "elaine:(to george) i can't believe it. i think i could get some coffee or not.\n",
      "\n",
      "jerry: you know i got a date with that, you know, it's not that you know.\n",
      "\n",
      "elaine: well, i'm sorry, i, i just need you to know that, you know what i do?\n",
      "\n",
      "jerry: well, you can't go.\n",
      "\n",
      "george: you\n"
     ]
    }
   ],
   "source": [
    "# Set preferences:\n",
    "gen_length = 400\n",
    "prime_word = 'jerry'\n",
    "\n",
    "# Define word for padding:\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "\n",
    "# Generate a TV script:\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab,\n",
    "                            token_dict, vocab_to_int[pad_word], gen_length)\n",
    "\n",
    "# Print the generated TV script:\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Save our favorite TV script*\n",
    "\n",
    "Once we have a script that we like (or find interesting), we can save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder 'tv_scripts' to store generated tv scripts:\n",
    "os.mkdir('tv_scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save script to a text file:\n",
    "generated_script_file = open('tv_scripts/generated_tv_script.txt','w')\n",
    "generated_script_file.write(generated_script)\n",
    "generated_script_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our generated TV script is not perfect...\n",
    "\n",
    "It's ok if our generated TV script doesn't make perfect sense, it should look like alternating lines of dialogue, here is one such example of a few generated lines:\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "As we can see, there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, we will have to use a smaller vocabulary (and discard uncommon words), or get more data.\n",
    "\n",
    "*Nota Bene:* The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes (for script generation, more than 1 MB of text is generally enough)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
